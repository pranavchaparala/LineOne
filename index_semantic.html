<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Voice Assistant</title>
<style>
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  body {
    margin: 0;
    padding: 0;
    min-height: 100vh;
    background: linear-gradient(180deg, #0a0a0f 0%, #1a1a2e 50%, #16213e 100%);
    font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Helvetica Neue', Arial, sans-serif;
    display: flex;
    flex-direction: column;
    overflow: hidden;
    box-shadow: inset 0 0 40px rgba(0, 122, 255, 0.15);
  }

  /* Fixed Input Bar at Bottom */
  .input-container {
    position: fixed;
    bottom: 0;
    left: 0;
    width: 100%;
    padding: 20px 25px;
    border-top: 1px solid rgba(255, 255, 255, 0.1);
    background: #101018; /* Darker background for fixed bar */
    z-index: 10;
  }

  .input-field {
    width: 100%;
    background: rgba(255, 255, 255, 0.05);
    border: 1px solid rgba(255, 255, 255, 0.1);
    border-radius: 12px;
    padding: 15px 20px;
    color: #fff;
    font-size: 16px;
    font-family: inherit;
    outline: none;
    transition: all 0.3s ease;
  }

  .input-field::placeholder {
    color: rgba(255, 255, 255, 0.3);
  }

  .input-field:focus {
    background: rgba(255, 255, 255, 0.08);
    border-color: rgba(0, 122, 255, 0.5);
    box-shadow: 0 0 20px rgba(0, 122, 255, 0.2);
  }

  /* Scrollable Transcript Area */
  .transcript-container {
    flex: 1;
    overflow-y: auto;
    padding: 20px 25px;
    -webkit-overflow-scrolling: touch;
    scrollbar-width: none;
    /* Added padding-bottom to clear the fixed input bar and mic section */
    padding-bottom: 120px; 
  }

  .transcript-container::-webkit-scrollbar {
    display: none;
  }

  .transcript-line {
    color: #e8e8e8;
    font-size: 18px;
    line-height: 1.6;
    margin-bottom: 16px;
    opacity: 0;
    transition: opacity 0.3s ease-out;
    word-wrap: break-word; 
  }

  .transcript-line.visible {
    opacity: 1;
  }

  .transcript-line.user {
    color: #ffffff;
    text-align: right;
  }

  .transcript-line.ai {
    color: #b8c5d6;
    text-align: left;
  }

  /* Microphone and Indicator Section */
  .bottom-section {
    position: fixed;
    bottom: 90px; /* Position above the fixed input bar */
    left: 0;
    width: 100%;
    padding: 0 25px 20px 25px;
    pointer-events: none; 
    background: transparent;
    z-index: 5;
  }

  .listening-indicator {
    color: #888;
    font-size: 13px;
    text-align: center;
    letter-spacing: 1px;
    margin-bottom: 15px;
    opacity: 0;
    transition: opacity 0.3s ease-in-out;
    font-weight: 500;
  }

  .listening-indicator.active {
    opacity: 1;
    animation: pulse-text 2s infinite;
  }

  @keyframes pulse-text {
    0%, 100% { opacity: 0.6; }
    50% { opacity: 1; }
  }

  .microphone-container {
    display: flex;
    justify-content: center;
    align-items: center;
    position: relative;
    height: 80px;
  }

  .microphone-button {
    width: 70px;
    height: 70px;
    border-radius: 50%;
    background: #fff;
    border: none;
    display: flex;
    justify-content: center;
    align-items: center;
    cursor: pointer;
    position: relative;
    z-index: 12;
    transition: all 0.3s ease;
    box-shadow: 0 4px 20px rgba(255, 255, 255, 0.3);
    pointer-events: auto; /* Allow microphone button to be clicked */
  }

  .microphone-button:hover {
    transform: scale(1.05);
    box-shadow: 0 6px 25px rgba(255, 255, 255, 0.4);
  }

  .microphone-button.listening {
    background: linear-gradient(135deg, #007aff 0%, #5ac8fa 100%);
    box-shadow: 0 0 30px rgba(0, 122, 255, 0.6);
    animation: mic-pulse 1.5s infinite;
  }

  @keyframes mic-pulse {
    0%, 100% { 
      box-shadow: 0 0 30px rgba(0, 122, 255, 0.6);
    }
    50% { 
      box-shadow: 0 0 50px rgba(0, 122, 255, 0.9);
    }
  }

  .microphone-icon {
    width: 32px;
    height: 32px;
    fill: #333;
    transition: fill 0.3s ease;
  }

  .microphone-button.listening .microphone-icon {
    fill: #fff;
  }

  .waveform {
    position: absolute;
    width: 90px;
    height: 90px;
    border-radius: 50%;
    background: rgba(0, 122, 255, 0.2);
    opacity: 0;
    z-index: 1;
  }

  .waveform.active {
    animation: waveform-pulse 1.5s infinite;
  }

  @keyframes waveform-pulse {
    0% {
      transform: scale(0.8);
      opacity: 0.4;
    }
    50% {
      transform: scale(1.2);
      opacity: 0.2;
    }
    100% {
      transform: scale(1.6);
      opacity: 0;
    }
  }

  .loading-dots {
    display: inline-block;
  }

  .loading-dots::after {
    content: '';
    animation: dots 1.5s steps(4, end) infinite;
  }

  @keyframes dots {
    0%, 20% { content: '.'; }
    40% { content: '..'; }
    60%, 100% { content: '...'; }
  }

  audio {
    display: none;
  }
</style>
</head>
<body>

<div class="transcript-container" id="transcriptContainer">
</div>

<div class="bottom-section">
  <div class="listening-indicator" id="listeningIndicator">
    AI LISTENING<span class="loading-dots"></span>
  </div>

  <div class="microphone-container">
    <div class="waveform" id="waveform"></div>
    <button class="microphone-button" id="microphoneButton">
      <svg class="microphone-icon" viewBox="0 0 24 24">
        <path d="M12 14c1.66 0 2.99-1.34 2.99-3L15 5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.3-3c0 3-2.54 5.1-5.3 5.1S6.7 14 6.7 11H5c0 3.41 2.72 6.23 6 6.72V21h2v-3.28c3.28-.48 6-3.3 6-6.72h-1.7z"/>
      </svg>
    </button>
  </div>
</div>

<div class="input-container">
  <input 
    type="text" 
    class="input-field" 
    id="queryInput" 
    placeholder="Type your query here or click the mic..."
  />
</div>

<audio id="loadingAudio" src="assets/ring.mp3"></audio>
<audio id="matchedAudio"></audio>

<script>
const microphoneButton = document.getElementById('microphoneButton');
const listeningIndicator = document.getElementById('listeningIndicator');
const waveform = document.getElementById('waveform');
const transcriptContainer = document.getElementById('transcriptContainer');
const matchedAudio = document.getElementById('matchedAudio');
const loadingAudio = document.getElementById('loadingAudio');
const queryInput = document.getElementById('queryInput');

let isProcessing = false;
let typingInterval = null;
let currentQuery = "";

// Simulated query for the mic button (replace with actual recording/transcription later)
const sampleQuery = "Hey, I've been working on this presentation for a new project, and I'm struggling with how to summarize the main points.";

// ðŸ”Š Ensure audio is loaded ðŸ”Š
loadingAudio.load(); 


// -----------------------------------------------------
// --- AUTOPLAY POLICY WORKAROUND (Using working logic) ---
// -----------------------------------------------------
function enableAudioContext() {
    // Attempts to play/pause silent audio on first user interaction to unlock context
    if (loadingAudio.paused) {
        loadingAudio.muted = true; 
        
        loadingAudio.play()
            .then(() => {
                loadingAudio.pause();
                loadingAudio.muted = false; 
                loadingAudio.currentTime = 0;
                console.log('Audio context successfully initialized/unlocked.');
            })
            .catch(e => {
                console.warn('Initial audio unlock failed (User interaction needed):', e.message);
            });
    }
}
// -----------------------------------------------------


// â­ Initial Setup and Interaction Event Listeners â­
document.addEventListener('DOMContentLoaded', () => {
    queryInput.focus();
    listeningIndicator.classList.add('active');
    
    // Attach the audio unlock function to the first user interaction
    queryInput.addEventListener('keydown', enableAudioContext, { once: true });
    microphoneButton.addEventListener('click', enableAudioContext, { once: true });
});

// Use Enter key to submit query
queryInput.addEventListener('keypress', (e) => {
  if (e.key === 'Enter' && !isProcessing) {
    const query = queryInput.value.trim();
    if (query) {
      startProcessing(query);
      queryInput.value = '';
    }
  }
});


function addTranscriptLine(text, className = 'user') {
  const line = document.createElement('div');
  line.className = `transcript-line ${className}`;
  line.textContent = text;
  transcriptContainer.appendChild(line);
  
  setTimeout(() => {
    line.classList.add('visible');
  }, 10);
  
  setTimeout(() => {
    transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
  }, 100);
}

function typeText(text, className, callback) {
  const words = text.split(' ');
  let currentLine = '';
  let wordIndex = 0;
  
  const line = document.createElement('div');
  line.className = `transcript-line ${className}`;
  line.textContent = '';
  transcriptContainer.appendChild(line);
  
  const typeInterval = setInterval(() => {
    if (wordIndex < words.length) {
      currentLine += (wordIndex > 0 ? ' ' : '') + words[wordIndex];
      line.textContent = currentLine;
      wordIndex++;
      
      transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
    } else {
      clearInterval(typeInterval);
      setTimeout(() => {
        line.classList.add('visible');
        transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
      }, 10);
      if (callback) callback();
    }
  }, 80); 
  
  typingInterval = typeInterval;
}


// --- Voice Input Logic ---
function startListening() {
  if (isProcessing) return;
  
  isProcessing = true;
  microphoneButton.classList.add('listening');
  waveform.classList.add('active');
  listeningIndicator.textContent = 'LISTENING...';
  
  // Simulate 1 second of "listening"
  setTimeout(() => {
    stopListeningVisuals();
    listeningIndicator.textContent = 'AI THINKING<span class="loading-dots"></span>';
    
    // Simulate user text appearing after speech-to-text
    typeText(sampleQuery, 'user', () => {
        // Proceed to network request with the transcribed query
        startProcessing(sampleQuery);
    });
  }, 1000); 
}

function stopListeningVisuals() {
    microphoneButton.classList.remove('listening');
    waveform.classList.remove('active');
}
// -------------------------


// --- Core Processing Logic (Used by both text input and voice input) ---
function startProcessing(query) {
  if (isProcessing) return;
  
  isProcessing = true;
  queryInput.disabled = true;
  currentQuery = query;
  
  // Update indicator text immediately before user text starts typing
  listeningIndicator.textContent = 'USER INPUT<span class="loading-dots"></span>';
  
  // Simulate typing the user's query (for text input) or confirm transcription (for voice input)
  typeText(query, 'user', () => {
    
    // Once user query is typed, AI starts thinking
    listeningIndicator.textContent = 'AI THINKING<span class="loading-dots"></span>';
    
    // ðŸ”Š Play the loading audio (ring.mp3) 
    loadingAudio.loop = true; 
    loadingAudio.play().catch(e => {
        console.error("Error playing loading audio:", e);
    });

    fetch('http://127.0.0.1:8000/search_text', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ query: query })
    })
    .then(response => response.json())
    .then(data => {
      
      // ðŸ›‘ Stop and reset the loading audio ðŸ›‘
      loadingAudio.pause();
      loadingAudio.currentTime = 0;
      loadingAudio.loop = false;
      
      if (data.matched_transcription) {
        listeningIndicator.textContent = 'AI RESPONDING<span class="loading-dots"></span>';
        
        typeText(data.matched_transcription, 'ai', () => {
          
          if (data.audio_file) {
            const audioUrl = `http://127.0.0.1:8000${data.audio_file}`;
            matchedAudio.src = audioUrl;
            matchedAudio.load(); 
            
            // â­ Working Audio Playback Logic from the "Press Start 2P" file â­
            matchedAudio.play()
              .then(() => {
                console.log('Audio playing successfully');
              })
              .catch(e => {
                // This will catch the Autoplay Policy error if the unlock failed
                console.error('Error playing AI response audio (Policy block?):', e);
              });
          }
          
          // Reset UI
          setTimeout(() => {
            stopProcessing();
          }, 1000);
        });
      } else {
        addTranscriptLine('Sorry, I couldn\'t process that request.', 'ai');
        stopProcessing();
      }
    })
    .catch(error => {
      console.error('Error:', error);
      addTranscriptLine('Sorry, I encountered a connection error. Please check the server.', 'ai');
      
      // ðŸ›‘ Stop and reset the loading audio on error ðŸ›‘
      loadingAudio.pause();
      loadingAudio.currentTime = 0;
      loadingAudio.loop = false;
      
      stopProcessing();
    });
  });
}

function stopProcessing() {
  if (typingInterval) {
    clearInterval(typingInterval);
    typingInterval = null;
  }
  
  isProcessing = false;
  currentQuery = "";
  stopListeningVisuals(); 
  listeningIndicator.classList.add('active');
  listeningIndicator.textContent = 'AI LISTENING<span class="loading-dots"></span>';
  queryInput.disabled = false;
  queryInput.focus();
}

// --- Mic Button Handler ---
microphoneButton.addEventListener('click', () => {
  if (!isProcessing) {
    startListening(); 
  }
});
</script>

</body>
</html>