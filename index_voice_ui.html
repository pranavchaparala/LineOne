<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Voice Assistant</title>
<style>
  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  body {
    margin: 0;
    padding: 0;
    min-height: 100vh;
    background: linear-gradient(180deg, #0a0a0f 0%, #1a1a2e 50%, #16213e 100%);
    font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Helvetica Neue', Arial, sans-serif;
    display: flex;
    flex-direction: column;
    overflow: hidden;
    box-shadow: inset 0 0 40px rgba(0, 122, 255, 0.15);
  }

  .input-container {
    /* Changed to fixed bottom position to be more chat-like */
    position: fixed;
    bottom: 0;
    left: 0;
    width: 100%;
    padding: 20px 25px;
    border-top: 1px solid rgba(255, 255, 255, 0.1);
    border-bottom: none; /* Removed redundant bottom border */
    background: #101018; /* Darker background for fixed bar */
    z-index: 10;
  }

  .input-field {
    width: 100%;
    background: rgba(255, 255, 255, 0.05);
    border: 1px solid rgba(255, 255, 255, 0.1);
    border-radius: 12px;
    padding: 15px 20px;
    color: #fff;
    font-size: 16px;
    font-family: inherit;
    outline: none;
    transition: all 0.3s ease;
  }

  .input-field::placeholder {
    color: rgba(255, 255, 255, 0.3);
  }

  .input-field:focus {
    background: rgba(255, 255, 255, 0.08);
    border-color: rgba(0, 122, 255, 0.5);
    box-shadow: 0 0 20px rgba(0, 122, 255, 0.2);
  }

  .transcript-container {
    flex: 1;
    overflow-y: auto;
    padding: 20px 25px;
    -webkit-overflow-scrolling: touch;
    scrollbar-width: none;
    /* Added padding-bottom to clear the fixed input bar */
    padding-bottom: 120px; 
  }

  .transcript-container::-webkit-scrollbar {
    display: none;
  }

  .transcript-line {
    color: #e8e8e8;
    font-size: 18px;
    line-height: 1.6;
    margin-bottom: 16px;
    opacity: 0;
    transition: opacity 0.3s ease-out;
    word-wrap: break-word; /* Ensure long lines wrap */
  }

  .transcript-line.visible {
    opacity: 1;
  }

  .transcript-line.user {
    color: #ffffff;
    text-align: right; /* User text aligns right for chat feel */
  }

  .transcript-line.ai {
    color: #b8c5d6;
    text-align: left;
  }

  .bottom-section {
    /* This section now contains the microphone and indicator */
    position: fixed;
    bottom: 90px; /* Position above the fixed input bar */
    left: 0;
    width: 100%;
    padding: 0 25px 20px 25px;
    pointer-events: none; /* Allow clicks to pass through to the transcript */
    background: transparent;
    z-index: 5;
  }
  
  /* Adjusted indicator position and style for the mic setup */
  .listening-indicator {
    color: #888;
    font-size: 13px;
    text-align: center;
    letter-spacing: 1px;
    margin-bottom: 15px;
    opacity: 0;
    transition: opacity 0.3s ease-in-out;
    font-weight: 500;
  }

  .listening-indicator.active {
    opacity: 1;
    animation: pulse-text 2s infinite;
  }

  .microphone-container {
    display: flex;
    justify-content: center;
    align-items: center;
    position: relative;
    height: 80px;
  }

  .microphone-button {
    width: 70px;
    height: 70px;
    border-radius: 50%;
    background: #fff;
    border: none;
    display: flex;
    justify-content: center;
    align-items: center;
    cursor: pointer;
    position: relative;
    z-index: 12; /* Bring button above input container (optional) */
    transition: all 0.3s ease;
    box-shadow: 0 4px 20px rgba(255, 255, 255, 0.3);
    pointer-events: auto; /* Allow microphone button to be clicked */
  }

  .microphone-button.listening {
    background: linear-gradient(135deg, #007aff 0%, #5ac8fa 100%);
    box-shadow: 0 0 30px rgba(0, 122, 255, 0.6);
    animation: mic-pulse 1.5s infinite;
  }
  /* Keyframes and other styles are unchanged */

  .loading-dots {
    display: inline-block;
  }

  .loading-dots::after {
    content: '';
    animation: dots 1.5s steps(4, end) infinite;
  }

  @keyframes dots {
    0%, 20% { content: '.'; }
    40% { content: '..'; }
    60%, 100% { content: '...'; }
  }

  audio {
    display: none;
  }
</style>
</head>
<body>

<div class="transcript-container" id="transcriptContainer">
  </div>

<div class="input-container">
  <input 
    type="text" 
    class="input-field" 
    id="queryInput" 
    placeholder="Type your query here or click the mic..."
  />
</div>

<div class="bottom-section">
  <div class="listening-indicator" id="listeningIndicator">
    AI LISTENING<span class="loading-dots"></span>
  </div>

  <div class="microphone-container">
    <div class="waveform" id="waveform"></div>
    <button class="microphone-button" id="microphoneButton">
      <svg class="microphone-icon" viewBox="0 0 24 24">
        <path d="M12 14c1.66 0 2.99-1.34 2.99-3L15 5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.3-3c0 3-2.54 5.1-5.3 5.1S6.7 14 6.7 11H5c0 3.41 2.72 6.23 6 6.72V21h2v-3.28c3.28-.48 6-3.3 6-6.72h-1.7z"/>
      </svg>
    </button>
  </div>
</div>

<audio id="matchedAudio"></audio>

<script>
const microphoneButton = document.getElementById('microphoneButton');
const listeningIndicator = document.getElementById('listeningIndicator');
const waveform = document.getElementById('waveform');
const transcriptContainer = document.getElementById('transcriptContainer');
const matchedAudio = document.getElementById('matchedAudio');
const queryInput = document.getElementById('queryInput');

let isListening = false;
let isProcessing = false;
let currentQuery = "";
let typingInterval = null;

// Simulated query from the image
const sampleQuery = "Hey, I've been working on this presentation for a new project, and I'm struggling with how to summarize the main points. It's a bit all over the place right now. The topic is about sustainable design, and I need it to sound clear and professional, but still approachable...";


// -----------------------------------------------------
// --- AUTOPLAY POLICY WORKAROUND ---
// -----------------------------------------------------
function enableAudioContext() {
    // This function attempts to unlock the browser's audio context
    // upon the first user interaction (typing, clicking, etc.)
    if (matchedAudio.paused) {
        matchedAudio.muted = true; // Play silently
        
        matchedAudio.play()
            .then(() => {
                matchedAudio.pause();
                matchedAudio.muted = false; // Reset mute
                matchedAudio.currentTime = 0;
                console.log('Audio context successfully initialized/unlocked.');
            })
            .catch(e => {
                console.warn('Initial audio unlock failed (User interaction needed):', e.message);
            });
    }
}
// -----------------------------------------------------

// ⭐ Initial Setup and Autoplay Unlock ⭐
document.addEventListener('DOMContentLoaded', () => {
    queryInput.focus();
    listeningIndicator.classList.add('active');
    
    // Attach the audio unlock function to the first user interaction
    // Typing:
    queryInput.addEventListener('keydown', enableAudioContext, { once: true });
    // Clicking the mic:
    microphoneButton.addEventListener('click', enableAudioContext, { once: true });
});

// Allow Enter key to submit query
queryInput.addEventListener('keypress', (e) => {
  if (e.key === 'Enter' && !isProcessing) {
    const query = queryInput.value.trim();
    if (query) {
      // 1. Add user query immediately (without typing simulation)
      addTranscriptLine(query, 'user'); 
      // 2. Process query
      processQuery(query);
      queryInput.value = '';
    }
  }
});


function addTranscriptLine(text, className = 'user') {
  const line = document.createElement('div');
  line.className = `transcript-line ${className}`;
  line.textContent = text;
  transcriptContainer.appendChild(line);
  
  // Trigger animation
  setTimeout(() => {
    line.classList.add('visible');
  }, 10);
  
  // Auto-scroll to bottom
  setTimeout(() => {
    transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
  }, 100);
}

function typeText(text, className, callback) {
  const words = text.split(' ');
  let currentLine = '';
  let wordIndex = 0;
  
  // Create a single line for this text
  const line = document.createElement('div');
  line.className = `transcript-line ${className}`;
  line.textContent = '';
  transcriptContainer.appendChild(line);
  
  const typeInterval = setInterval(() => {
    if (wordIndex < words.length) {
      currentLine += (wordIndex > 0 ? ' ' : '') + words[wordIndex];
      line.textContent = currentLine;
      wordIndex++;
      // Auto-scroll as text is typed
      transcriptContainer.scrollTop = transcriptContainer.scrollHeight; 
    } else {
      clearInterval(typeInterval);
      // Make it visible after typing is complete
      setTimeout(() => {
        line.classList.add('visible');
      }, 10);
      if (callback) callback();
    }
  }, 80); // Adjust typing speed
  
  typingInterval = typeInterval;
}

// Function to handle the microphone click (simulates speech-to-text)
function startListening() {
  if (isProcessing) return;
  
  isListening = true;
  isProcessing = true;
  microphoneButton.classList.add('listening');
  listeningIndicator.textContent = 'LISTENING...';
  listeningIndicator.classList.add('active');
  waveform.classList.add('active');
  
  // Simulate 1 second of "listening"
  setTimeout(() => {
    stopListeningVisuals();
    listeningIndicator.textContent = 'PROCESSING...';
    // Simulate user text appearing after speech-to-text
    typeText(sampleQuery, 'user', () => {
        currentQuery = sampleQuery;
        // Proceed to network request
        processQuery(sampleQuery);
    });
  }, 1000); 
}

// Helper to turn off visual indicators after speech input is captured
function stopListeningVisuals() {
    isListening = false;
    microphoneButton.classList.remove('listening');
    waveform.classList.remove('active');
}

// Function to handle the network request and AI response
async function processQuery(query) {
  if (isProcessing && currentQuery === query) return;
  
  isProcessing = true;
  currentQuery = query;
  queryInput.disabled = true;
  
  // Set processing indicator
  listeningIndicator.textContent = 'AI THINKING<span class="loading-dots"></span>';
  
  fetch('http://127.0.0.1:8000/search_text', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ query: query })
  })
  .then(response => response.json())
  .then(data => {
    
    if (data.matched_transcription) {
      // Type out the AI response
      listeningIndicator.textContent = 'AI RESPONDING<span class="loading-dots"></span>';
      
      typeText(data.matched_transcription, 'ai', () => {
        
        if (data.audio_file) {
            const audioUrl = `http://127.0.0.1:8000${data.audio_file}`;
            matchedAudio.src = audioUrl;
            
            // Define the playback handler once
            const playbackHandler = () => {
                matchedAudio.play()
                  .then(() => {
                    console.log('Audio playing successfully');
                  })
                  .catch(e => {
                    // This is the Autoplay Policy catch
                    console.error('Playback Failed (Browser Policy):', e.message);
                  })
                  .finally(() => {
                    // Clean up the listener after the first attempt
                    matchedAudio.removeEventListener('canplaythrough', playbackHandler); 
                  });
            };
            
            // 🔊 ATTACH LISTENER: Wait for the file to be ready to play
            matchedAudio.addEventListener('canplaythrough', playbackHandler);
            
            matchedAudio.load(); // Start loading the audio
        }
        
        // Reset UI
        setTimeout(() => {
          stopProcessing();
        }, 1000);
      });
    } else {
        addTranscriptLine('Sorry, I couldn\'t process that request.', 'ai');
        stopProcessing();
    }
  })
  .catch(error => {
    console.error('Error:', error);
    addTranscriptLine('Sorry, I encountered a connection error. Please check the server.', 'ai');
    stopProcessing();
  });
}

function stopProcessing() {
  if (typingInterval) {
    clearInterval(typingInterval);
    typingInterval = null;
  }
  
  isProcessing = false;
  currentQuery = "";
  stopListeningVisuals(); // Ensure visuals are off
  listeningIndicator.classList.add('active'); // Keep indicator active when listening
  listeningIndicator.textContent = 'AI LISTENING<span class="loading-dots"></span>';
  queryInput.disabled = false;
  queryInput.focus();
}

microphoneButton.addEventListener('click', () => {
  if (!isProcessing) {
    // If the process is initiated by mic, we call startListening (simulating the whole process)
    startListening(); 
  }
});
</script>

</body>
</html>